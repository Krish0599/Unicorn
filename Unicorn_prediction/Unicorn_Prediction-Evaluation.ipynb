{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for evaluation of the Unicorn prediction model. The dataset provided has already known unicorns in it. The confusion matrix and accuracy is used to evaluate the Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import binarize\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicorns = []\n",
    "with open('D:\\\\cmpe256\\\\Team project\\\\evaluation\\\\unicorns.csv','r') as unicornfile:\n",
    "    reader = csv.reader(unicornfile)\n",
    "    reader.__next__()\n",
    "    for row in reader:\n",
    "        unicorns.append(row[0])\n",
    "unicornfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\\\cmpe256\\\\Team project\\\\evaluation\\\\startups6.csv','r') as file1:\n",
    "    reader = csv.reader(file1)\n",
    "    reader.__next__()\n",
    "    with open('D:\\\\cmpe256\\\\Team project\\\\evaluation\\\\startups5.csv','w',newline=\"\") as file2:\n",
    "        writer = csv.writer(file2)\n",
    "        writer.writerow(['Name','Start date','Funding','Categories','Number of Articles','Status'])        \n",
    "        for row in reader:\n",
    "            if not row[1].strip():\n",
    "                continue\n",
    "            if row[2] == '':\n",
    "                continue\n",
    "            if row[3] == '':\n",
    "                continue\n",
    "            if row[4] == '':\n",
    "                continue\n",
    "            if row[0] in unicorns:\n",
    "                row.append('Unicorn')\n",
    "            else:\n",
    "                row.append('Not Unicorns')\n",
    "            row.append('')\n",
    "            writer.writerow(row)\n",
    "    file2.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "article_cnt = []\n",
    "investments = []\n",
    "founding_dates = []\n",
    "names = []\n",
    "with open('D:\\\\cmpe256\\\\Team project\\\\evaluation\\\\startups5.csv','r') as file1:\n",
    "    reader = csv.reader(file1)\n",
    "    reader.__next__()\n",
    "    for row in reader:\n",
    "        categories.append(row[3].split(','))\n",
    "        names.append(row[0])\n",
    "        article_cnt.append(row[4])\n",
    "        investments.append(row[2])\n",
    "        founding_dates.append(row[1])\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    for i,name in enumerate(category):\n",
    "        category[i] = name.strip().replace(' ','_').replace('-','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,fdate in enumerate(founding_dates):\n",
    "    fdate = fdate.strip()\n",
    "    founding_dates[i] = fdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,article in enumerate(article_cnt):\n",
    "    article = article.strip()\n",
    "    article = article.replace(\",\",\"\")\n",
    "    article_cnt[i] = article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investment amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,money in enumerate(investments):\n",
    "    money = money.replace('$','')\n",
    "    money = money.replace(',','')\n",
    "    investments[i] = money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(categories):\n",
    "    corpus = []\n",
    "    for category in categories:\n",
    "        doc = \"\"\n",
    "        for name in category:\n",
    "            doc = doc + name + \" \"\n",
    "        corpus.append(doc.strip())\n",
    "    return corpus\n",
    "\n",
    "SMOOTHING = 25\n",
    "def cosine(X):\n",
    "    return cosine_similarity(X)\n",
    "\n",
    "def cosine_smooth(X):\n",
    "    similarity_matrix = cosine_similarity(X)\n",
    "    i = 0\n",
    "    while(i<X.shape[0]):\n",
    "        j = 0\n",
    "        while(j<X.shape[0]):\n",
    "            overlap = np.dot(binarize(X.todense()[i]), binarize(X.todense()[j]).T)[0, 0]\n",
    "            similarity_matrix[i][j] *= (overlap/(overlap + SMOOTHING))\n",
    "            j+=1\n",
    "        i+=1\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = generate_corpus(categories)\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine(X)\n",
    "similarity_matrix = similarity_matrix.tolist()\n",
    "\n",
    "i=0\n",
    "for sim in similarity_matrix:\n",
    "    del sim[i]\n",
    "    i+=1\n",
    "similarity_matrix = np.array(similarity_matrix)\n",
    "\n",
    "similarity_index = []\n",
    "for sim in similarity_matrix:\n",
    "    similarity_index.append(statistics.mean(sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map = {'jan':1,\n",
    "            'feb':2,\n",
    "            'mar':3,\n",
    "            'apr':4,\n",
    "            'may':5,\n",
    "            'jun':6,\n",
    "            'jul':7,\n",
    "            'aug':8,\n",
    "            'sep':9,\n",
    "            'oct':10,\n",
    "            'nov':11,\n",
    "            'dec':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "duration = []\n",
    "for sdate in founding_dates:\n",
    "    day = 1\n",
    "    month = 1\n",
    "    if len(sdate) > 4:\n",
    "        month = month_map[sdate[:3].lower()]\n",
    "        year = sdate[-4:]\n",
    "    else:\n",
    "        year = sdate\n",
    "            \n",
    "    startdate = datetime.datetime(int(year),int(month),int(day),0,0,0,0)\n",
    "    difference = relativedelta(now,startdate)\n",
    "    duration.append(difference.years) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_score = []\n",
    "for count,years in zip(article_cnt,duration):\n",
    "    rate = float(count)/float(years)\n",
    "    popularity_score.append(rate*0.6 + float(count)*0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investment Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_map = {'K':1000,\n",
    "             'M':1000000,\n",
    "             'B':1000000000}\n",
    "for i,money in enumerate(investments):\n",
    "    money = float(money[:-1]) * number_map[money[-1:]]\n",
    "    investments[i] = money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_rate = 0.1\n",
    "i = 0\n",
    "for money,years in zip(investments,duration):\n",
    "    new_money = int(money)*math.exp(-decay_rate * years)\n",
    "    investments[i] = new_money\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing 2 (Data scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(len(names)):\n",
    "    dataset.append([similarity_index[i],popularity_score[i],investments[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(1,10))\n",
    "scaler.fit(dataset)\n",
    "scaled_data = scaler.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modeling (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for name,data in zip(names,scaled_data):\n",
    "    score = (data[1]*data[2]) / data[0]\n",
    "    result_list.append([name,score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_unicorns = []\n",
    "with open('D:\\\\cmpe256\\\\Team project\\\\unicorns.csv','r') as file1:\n",
    "    reader1 = csv.reader(file1)\n",
    "    reader1.__next__()\n",
    "    for row in reader1:\n",
    "        existing_unicorns.append(row[0])\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "actual_unicorns=[]\n",
    "print(len(result_list))\n",
    "for row in result_list:\n",
    "    if row[0] in existing_unicorns:\n",
    "        actual_unicorns.append(row[0])\n",
    "print(len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(result_list,key=lambda l:l[1],reverse=True))\n",
    "print(len(result_list))\n",
    "\"\"\"for name,data in zip(names,scaled_data):\n",
    "    print(name,data)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lyft', 'Spotify', 'Instacart', 'Oscar Health', 'STILT', 'Postmates', 'Coinbase', 'MaestroQA', 'Medium']\n"
     ]
    }
   ],
   "source": [
    "predicted_unicorns = []\n",
    "sorted_list = sorted(result_list,key=lambda l:l[1],reverse=True)\n",
    "temp = sorted_list[:9]\n",
    "for row in temp:\n",
    "    predicted_unicorns.append(row[0])\n",
    "print(predicted_unicorns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positive: 6\n",
      "false_positive: 3\n",
      "true_negative: 125\n",
      "false_negative: 3\n",
      "0.6666666666666666\n",
      "0.9765625\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "for name in actual_unicorns:\n",
    "    if name in predicted_unicorns:\n",
    "        true_positive+=1\n",
    "false_positive = len(predicted_unicorns) - true_positive\n",
    "true_negative = len(result_list) - len(predicted_unicorns) - false_positive\n",
    "false_negative = false_positive\n",
    "print(\"true_positive:\",true_positive)\n",
    "print(\"false_positive:\",false_positive)\n",
    "print(\"true_negative:\",true_negative)\n",
    "print(\"false_negative:\",false_negative)\n",
    "\n",
    "ppv = true_positive / (true_positive+false_positive)\n",
    "npv = true_negative / (false_negative+true_negative)\n",
    "print(ppv)\n",
    "print(npv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
